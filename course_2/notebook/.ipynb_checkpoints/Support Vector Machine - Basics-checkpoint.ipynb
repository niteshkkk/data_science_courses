{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* SVM Basics\n",
    "* Advantages and Disadvantages\n",
    "* Maximum margin classifier\n",
    "* Support vector classifier\n",
    "* Support vector machine\n",
    "* kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines - Basics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem to be solved here is one of supervised binary classification. That is, we wish to categorise new unseen objects into two separate groups based on their properties and a set of known examples, which are already categorised. A good example of such a system is classifying a set of new documents into positive or negative sentiment groups, based on other documents which have already been classified as positive or negative. Similarly, we could classify new emails into spam or non-spam, based on a large corpus of documents that have already been marked as spam or non-spam by humans. SVMs are highly applicable to such situations. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Support Vector Machine models the situation by creating a feature space, \n",
    "which is a finite-dimensional vector space, each dimension of which represents a \n",
    "\"feature\" of a particular object. In the context of spam or document classification, \n",
    "each \"feature\" is the prevalence or importance of a particular word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Goal of SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the SVM is to train a model that assigns new unseen objects into a particular category. It achieves this by creating a linear partition of the feature space into two categories. Based on the features in the new unseen objects (e.g. documents/emails), it places an object \"above\" or \"below\" the separation plane, leading to a categorisation (e.g. spam or non-spam). This makes it an example of a non-probabilistic linear classifier. It is non-probabilistic, because the features in the new objects fully determine its location in \n",
    "feature space and there is no stochastic element involved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, much of the benefit of SVMs comes from the fact that they are not restricted to being linear classifiers. Utilising a technique known as the kernel trick they can become much more flexible by introducing various types of non-linear decision boundaries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical Fundamental"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formally, in mathematical language, SVMs construct linear separating hyperplanes in high-dimensional vector spaces. Data points are viewed as $(\\vec{x}, y)$ tuples, $\\vec{x} = (x_1, \\ldots, x_p)$ where the $x_j$ are the feature values and y is the classification (usually given as +1 or -1). Optimal classification occurs when such hyperplanes provide maximal distance to the nearest training data points. Intuitively, this makes sense, as if the points are well separated, the classification between two groups is much clearer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### However, if in a feature space some of the sets are not linearly separable (i.e. they overlap!), then it is necessary to perform a mapping of the original feature space to a higher-dimensional space, in which the separation between the groups is clear, or at least clearer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages and Disadvantages of SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages of SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### High-Dimensionality - The SVM is an effective tool in high-dimensional spaces, which is particularly applicable to document classification and sentiment analysis where the dimensionality can be extremely large (≥106).\n",
    "\n",
    "#### Memory Efficiency - Since only a subset of the training points are used in the actual decision process of assigning new members, only these points need to be stored in memory (and calculated upon) when making decisions.\n",
    "\n",
    "#### Versatility - Class separation is often highly non-linear. The ability to apply new kernels allows substantial flexibility for the decision boundaries, leading to greater classification performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disadvantages of SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### p>n \n",
    "\n",
    "##### In situations where the number of features for each object (p) exceeds the number of training data samples (n), SVMs can perform poorly. This can be seen intuitively, as if the high-dimensional feature space is much larger than the samples, then there are less effective support vectors on which to support the optimal linear hyperplanes, leading to poorer classification performance as new unseen samples are added.\n",
    "\n",
    "#### Non-Probabilistic \n",
    "\n",
    "##### Since the classifier works by placing objects above and below a classifying hyperplane, there is no direct probabilistic interpretation for group membership. However, one potential metric to determine \"effectiveness\" of the classification is how far from the decision boundary the new point is.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Separating Hyperplanes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we consider a real-valued p-dimensional feature space, known mathematically as $\\mathbb{R}^p$, then our linear separating hyperplane is an affine p-1 dimensional space embedded within it.\n",
    "\n",
    "For the case of p=2 this hyperplane is simply a one-dimensional straight line, which lives in the larger two-dimensional plane, whereas for p=3 the hyerplane is a two-dimensional plane that lives in the larger three-dimensional feature space (see Fig 1 and Fig 2):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://s3.amazonaws.com/quantstart/media/images/qs-svm-0001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we consider an element of our p-dimensional feature space, i.e. $\\vec{x}=(x_1,...,x_p) \\in \\mathbb{R}^p$, then we can mathematically define an affine hyperplane by the following equation:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "b_0 + b_1 x_1 + ... + b_p x_p = 0\n",
    "\\end{eqnarray}\n",
    "$$b_0 \\neq 0$$ gives us an affine plane (i.e. it does not pass through the origin). We can use a more succinct notation for this equation by introducing the summation sign:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "b_0 + \\sum^{p}_{j=1} b_j x_j = 0\n",
    "\\end{eqnarray}\n",
    "Notice however that this is nothing more than a multi-dimensional dot product (or, more generally, an inner product), and as such can be written even more succinctly as:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\vec{b} \\cdot \\vec{x} + b_0 = 0\n",
    "\\end{eqnarray}\n",
    "If an element $\\vec{x} \\in \\mathbb{R}^p$ satisfies this relation then it lives on the p-1-dimensional hyperplane. This hyperplane splits the p-dimensional feature space into two classification regions (see Fig 3):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://s3.amazonaws.com/quantstart/media/images/qs-svm-0002.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elements $\\vec{x}$ above the plane satisfy:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\vec{b} \\cdot \\vec{x} + b_0 > 0\n",
    "\\end{eqnarray}\n",
    "While those below it satisfy:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\vec{b} \\cdot \\vec{x} + b_0 < 0\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The key point here is that it is possible for us to determine which side of the plane any element $\\vec{x}$ will fall on by calculating the sign of the expression $\\vec{b} \\cdot \\vec{x} + b_0$.  \n",
    "\n",
    "##### This concept will form the basis of a supervised classification technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuing with our example of email spam filtering, we can think of our classification problem (say) as being provided with a thousand emails (n=1000), each of which is marked spam (+1) or non-spam (−1). In addition, each email has an associated set of keywords (i.e. separating the words on spacing) that provide features. Hence if we take the set of all possible keywords from all of the emails (and remove duplicates), we will be left with p keywords in total.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we translate this into a mathematical problem, the standard setup for a supervised classification procedure is to consider a set of n training observations, $\\vec{x}_i$, each of which is a p-dimensional vector of features. Each training observation has an associated class label, $y_i \\in \\{ -1,1 \\}$. Hence we can think of n pairs of training observations ($\\vec{x}_i, y_i$) representing the features and class labels (keyword lists and spam/non-spam). In addition to the training observations we can provide test observations, $\\vec{x}^{*} = (x^{*}_1, ... , x^{*}_p$) that are later used to test the performance of the classifiers. In our spam example, these test observations would be new emails that have not yet been seen.\n",
    "\n",
    "Our goal is to develop a classifier based on provided training observations that will correctly classify subsequent test observations using only their feature values. This translates into being able to classify an email as spam or non-spam solely based on the keywords contained within it.\n",
    "\n",
    "We will initially suppose that it is possible, via a means yet to be determined, to construct a hyperplane that separates training data perfectly according to their class labels (see Figs 4 and 5). This would mean cleanly separating spam emails from non-spam emails solely by using specific keywords. The following diagram is only showing p=2, while for keyword lists we may have p>10^6. Hence Figs 4 and 5 are only representative of the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://s3.amazonaws.com/quantstart/media/images/qs-svm-0003.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ####                              Fig 4: Multiple separating hyperplanes; Fig 5: Perfect separation of class data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This translates into a mathematical separating property of:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\vec{b} \\cdot \\vec{x}_i + b_0 > 0,\\enspace\\text{if}\\enspace y_i = 1\n",
    "\\end{eqnarray}\n",
    "and\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\vec{b} \\cdot \\vec{x}_i + b_0 < 0,\\enspace\\text{if}\\enspace y_i = -1\n",
    "\\end{eqnarray}\n",
    "\n",
    "This basically states that if each training observation is above or below the separating hyperplane, according to the geometric equation which defines the plane, then its associated class label will be +1 or -1. Thus we have developed a simple classification process. We assign a test observation to a class depending upon which side of the hyperplane it is located on.\n",
    "\n",
    "#### The introduces the concept of the maximal margin hyperplane and a classifier built on it, known as the maximal margin classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deriving the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It is important to note that the separating hyperplanes are not unique, since it is possible to slightly translate or rotate such a plane without touching any training observations. So, not only do we need to know how to construct such a plane, but we also need to determine the most optimal. \n",
    "###### See Below figures.\n",
    "\n",
    "\n",
    "![](https://s3.amazonaws.com/quantstart/media/images/qs-svm-0003.png)\n",
    "\n",
    " \n",
    "\n",
    "#### This motivates the concept of the maximal margin hyperplane (MMH), which is the separating hyperplane that is farthest from any training observations, and is thus \"optimal\".\n",
    "\n",
    "### How do we find the maximal margin hyperplane? \n",
    "\n",
    "Firstly, we compute the perpendicular distance from each training observation $\\vec{x}_i$ for a given separating hyperplane. The smallest perpendicular distance to a training observation from the hyperplane is known as the margin. The MMH is the separating hyperplane where the margin is the largest. This guarantees that it is the farthest minimum distance to a training observation.\n",
    "\n",
    "The classification procedure is then just simply a case of determining which side a test observation falls on.\n",
    "#### Such a classifier is known as a maximimal margin classifier (MMC)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://s3.amazonaws.com/quantstart/media/images/qs-svm-0004.png)\n",
    "##### Fig 6: Maximal margin hyperplane with support vectors (A, B and C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the key features of the MMC (and subsequently SVC and SVM) is that the location of the MMH only depends on the support vectors, which are the training observations that lie directly on the margin (but not hyperplane) boundary (see points A, B and C in Fig 6). This means that the location of the MMH is NOT dependent upon any other training observations.\n",
    "\n",
    "#### Thus a potential drawback of the MMC is that its MMH (and thus its classification performance) can be extremely sensitive to the support vector locations. \n",
    "#### However, it is also partially this feature that makes the SVM an attractive computational tool, as we only need to store the support vectors in memory once it has been \"trained\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Classifiers ( Soft Margin Classifier )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the problems with MMC is that they can be extremely sensitive to the addition of new training observations. Consider Figs 8 and 9. In Fig 8 it can be seen that there exists a MMH perfectly separating the two classes. However, in Fig 9 if we add one point to the +1 class we see that the location of the MMH changes substantially. Hence in this situation the MMH has clearly been over-fit:\n",
    "\n",
    "![](https://s3.amazonaws.com/quantstart/media/images/qs-svm-0006.png)\n",
    "\n",
    "##### Figs 8 and 9: Addition of a single point dramatically changes the MMH line\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we could consider a classifier based on a separating hyperplane that doesn't perfectly separate the two classes, but does have a greater robustness to the addition of new invididual observations and has a better classification on most of the training observations. \n",
    "##### This comes at the expense of some misclassification of a few training observations.\n",
    "\n",
    "#### This is how a support vector classifier or soft margin classifier works. \n",
    "##### A SVC allows some observations to be on the incorrect side of the margin (or hyperplane), hence it provides a \"soft\" separation. The following figures 10 and 11 demonstrate observations being on the wrong side of the margin and the wrong side of the hyperplane respectively:\n",
    "\n",
    "\n",
    "![](https://s3.amazonaws.com/quantstart/media/images/qs-svm-0007.png)\n",
    "##### Figures 10 and 11 Observations on the wrong side of the margin and hyperplane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As before, an observation is classified depending upon which side of the separating hyperplane it lies on, but some points may be misclassified.\n",
    "\n",
    "It is instructive to see how the optimisation procedure differs from that described above for the MMC. We need to introduce new parameters, namely n $\\epsilon_i$ values (known as the slack values) and a parameter C, known as the budget. We wish to maximise M, across $b_1, ..., b_p,\\epsilon_1,..,\\epsilon_n$ such that:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\sum^{p}_{j=1} b^2_j = 1\n",
    "\\end{eqnarray}\n",
    "and\n",
    "\n",
    "\\begin{eqnarray}\n",
    "y_i \\left( \\vec{b} \\cdot \\vec{x} + b_0 \\right) \\geq M (1 - \\epsilon_i), \\quad \\forall i = 1,...,n\n",
    "\\end{eqnarray}\n",
    "and\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\\epsilon_i \\geq 0, \\quad \\sum^{n}_{i=1} \\epsilon_i \\leq C\n",
    "\\end{eqnarray}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where C, the budget, is a non-negative \"tuning\" parameter. M still represents the margin and the slack variables $\\epsilon_i$ allow the individual observations to be on the wrong side of the margin or hyperplane.\n",
    "\n",
    "In essence the $\\epsilon_i$ tell us where the ith observation is located relative to the margin and hyperplane. For $\\epsilon_i=0$ it states that the $x_i$ training observation is on the correct side of the margin. For $\\epsilon_i>0$ we have that $x_i$ is on the wrong side of the margin, while for $\\epsilon_i>1$ we have that $x_i$ is on the wrong side of the hyperplane.\n",
    "\n",
    "C collectively controls how much the individual $\\epsilon_i$ can be modified to violate the margin. C=0 implies that $\\epsilon_i=0, \\forall$ i and thus no violation of the margin is possible, in which case (for separable classes) we have the MMC situation.\n",
    "\n",
    "For C>0 it means that no more than C observations can violate the hyperplane. As C increases the margin will widen. See Fig 12 and 13 for two differing values of C:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://s3.amazonaws.com/quantstart/media/images/qs-svm-0008.png)\n",
    "#### Figs 12 and 13: Different values of the tuning parameter C\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we choose C in practice? Generally this is done via cross-validation. In essence C is the parameter that governs the bias-variance trade-off for the SVC. A small value of C means a low bias, high variance situation. A large value of C means a high bias, low variance situation.\n",
    "\n",
    "As before, to classify a new test observation $x^{*}$ we simply calculate the sign of $f(\\vec{x}^{*})= \\vec{b} \\cdot \\vec{x}^{*} + b_0.$\n",
    "\n",
    "This is all well and good for classes that are linearly (or nearly linearly) separated. \n",
    "#### However, what about separation boundaries that are non-linear? \n",
    "#### How do we deal with those situations? \n",
    "#### This is where we can extend the concept of support vector classifiers to support vector machines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machines allows for a non-linear decision boundaries.\n",
    "#### The following figures of 14 and 15 has NO linearly separable boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://s3.amazonaws.com/quantstart/media/images/qs-svm-0009.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dataset with a non-linear decision boundary can be a mapped to a linear decision boundary in the Higher dimensional space.\n",
    "\n",
    "In the below diagram the figure on the left is not linearly separable in 2-D space but becomes linearly separable when transformed to a 3-D space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://qph.ec.quoracdn.net/main-qimg-8a4a30421342fedb9bdda38fbd2529a8.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example of a labeled data inseparable in 2-Dimension is separable in 3-Dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The major advantage of SVMs is that they allow a non-linear enlargening of the feature space, while still retaining a significant computational efficiency, using a process known as the \"kernel trick\", "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernels and Kernel Trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we assumed a linear separable set of training data. Nevertheless, this is only the case in very few real-world applications. Now the kernel function comes to handy as a remedy, as an implicit mapping of the input space into a linear separable feature space, where our linear classifiers are again applicable.\n",
    "Below section explain the Feature space mapping as well as the Kernel Trick."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Space Mapping and kernel trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start with an example. Consider a non-linear mapping function\n",
    "Φ : I = R\n",
    "2 → F = R\n",
    "3\n",
    "from the 2-dimensional input space I into the 3-\n",
    "dimensional feature space F, which is defined in the following way:\n",
    "Φ(~x) = (x\n",
    "2\n",
    "1\n",
    ",\n",
    "√\n",
    "2x1x2, x2\n",
    "2\n",
    ")\n",
    "T\n",
    ". (19)\n",
    "Taking the equation for a separating hyperplane Eq.(1) into account we get\n",
    "a linear function in R\n",
    "3\n",
    ":\n",
    "~w\n",
    "TΦ(~x) = w1x\n",
    "2\n",
    "1 + w2\n",
    "√\n",
    "2x1x2 + w3x\n",
    "2\n",
    "2 = 0. (20)\n",
    "It is worth mentioning, that Eq.(20) is an elliptic function when set to a\n",
    "constant c and evaluated in R\n",
    "2\n",
    ". Hence, with an appropriate mapping function\n",
    "we can use our linear classifier in F on a transformed version of the data to\n",
    "get a non-linear classifier in I with no effort. After mapping our non-linear\n",
    "separable data into a higher dimensional space we can find a linear separating hyperplane. For an intuitive understanding, considder the below figure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://qph.ec.quoracdn.net/main-qimg-8a4a30421342fedb9bdda38fbd2529a8.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, by simply applying our linear maximum margin classifier to a mapped\n",
    "data set, we can reformulate our dual Lagrangian of our optimisation problem\n",
    "of Eq.(13)\n",
    "LD(~α) = X\n",
    "l\n",
    "i=1\n",
    "αi −\n",
    "X\n",
    "l\n",
    "i=1\n",
    "X\n",
    "l\n",
    "j=1\n",
    "αiαjyiyjΦ(~xi)\n",
    "TΦ(~xj ), (21)\n",
    "the optimal weight vector Eq.(14)\n",
    "~wo =\n",
    "X\n",
    "l\n",
    "i=1\n",
    "αi,oyiΦ(~xi), (22)\n",
    "the optimal hyperplane Eq.(15)\n",
    "~w\n",
    "T\n",
    "o\n",
    "~x + bo =\n",
    "X\n",
    "l\n",
    "i=1\n",
    "αi,oyiΦ(~xi)\n",
    "TΦ(~x) + bo = 0; (23)\n",
    "and the optimal decision function Eq.(16)\n",
    "g(~x) = sgn( ~w\n",
    "T\n",
    "o\n",
    "~x + bo) = sgn X\n",
    "l\n",
    "i=1\n",
    "αi,oyiΦ(~xi)\n",
    "TΦ(~x) + bo\n",
    "!\n",
    ". (24)\n",
    "From Eq.(22) follows, that our weight vector of the optimal hyperplane in F\n",
    "can be represented only by data points. Note also, that both, Eq.(23) and\n",
    "Eq.(24), only depend on the mapped data through dot products in some feature\n",
    "space F. The explicit coordinates in F and even the mapping function\n",
    "Φ become unnecessary when we define a function K(~xi\n",
    ", ~x) = Φ(~xi)\n",
    "TΦ(~x), the so called kernel function, which directly calculates the value of the dot product\n",
    "of the mapped data points in some feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Function and Kernel Trick Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example\n",
    "of a kernel function K demonstrates the calculation of the dot product in\n",
    "the feature space using K(~x, ~z) =\n",
    "~x T ~z\u00012\n",
    "and inducing the mapping function\n",
    "Φ(~x) = (x\n",
    "2\n",
    "1\n",
    ",\n",
    "√\n",
    "2x1x2, x2\n",
    "2\n",
    ")\n",
    "T of Eq.(19):\n",
    "~x = (x1, x2)\n",
    "~z = (z1, z2)\n",
    "K(~x, ~z) =\n",
    "~x T\n",
    "~z\u00012\n",
    "= (x1z1 + x2z2)\n",
    "2\n",
    "= (x\n",
    "2\n",
    "1\n",
    "z\n",
    "2\n",
    "1 + 2x1z1x2z2 + x\n",
    "2\n",
    "2\n",
    "z\n",
    "2\n",
    "2\n",
    ")\n",
    "= (x\n",
    "2\n",
    "1\n",
    ",\n",
    "√\n",
    "2x1x2, x2\n",
    "2\n",
    ")\n",
    "T\n",
    "(z\n",
    "2\n",
    "1\n",
    ",\n",
    "√\n",
    "2z1z2, z2\n",
    "2\n",
    ")\n",
    "= φ(~x)\n",
    "T φ(~z)\n",
    "The advantage of such a kernel function is that the complexity of the optimisation\n",
    "problem remains only dependent on the dimensionality of the input\n",
    "space and not of the feature space. Therefore, it is possible to operate in a\n",
    "theoretical feature space of infinite height.\n",
    "We can solve our dual Lagrangian of our optimisation problem in Eq.(21)\n",
    "using the kernel function K:\n",
    "LD(~α) = X\n",
    "l\n",
    "i=1\n",
    "αi −\n",
    "X\n",
    "l\n",
    "i=1\n",
    "X\n",
    "l\n",
    "j=1\n",
    "αiαjyiyjK(~xi\n",
    ", ~xj ) (25)\n",
    "With the dual representation of the optimal weight vector Eq.(22) of the\n",
    "decision surface in the feature space F, we can finally also reformulate the\n",
    "equation of our optimal separating hyperplane:\n",
    "~w\n",
    "T\n",
    "o\n",
    "~x + bo =\n",
    "X\n",
    "l\n",
    "i=1\n",
    "αi,oyiK(~xi\n",
    ", ~x) + bo = 0, (26)\n",
    "where αi,o are the optimal Lagrange multipliers obtained from maximising\n",
    "Eq. (25) and bo the optimal perpendicular distance from the origin, calculated\n",
    "according to Eq.(18), but now with ~wo and ~x(s)\n",
    "in F.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to know the appropriate kernel ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice of a Kernel depends on the problem at hand because it depends on what we are trying to model.\n",
    "#### A polynomial kernel, for example, allows us to model feature conjunctions up to the order of the polynomial. Radial basis functions allows to pick out circles (or hyperspheres) – in constrast with the Linear kernel, which allows only to pick out lines (or hyperplanes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some tips to find the appropriate kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, we do need to determine which Kernel function will be appropriate. However, we do not need to know it first.\n",
    "#### First, we have to realize that a linear decision boundary is not going to work. This is realized when you see a poor model accuracy, and some data visualization can be used, if possible. \n",
    "Upon realizing that linear boundary is not going to work, we go for a Kernel trick. All Kernels will lead to a non-linear, and possibly better, decision boundary. Next section gives an exhaustive list of choices. But there is no direct way to know which Kernel function will be the best choice.\n",
    "\n",
    "#### Conventional Machine Learning model optimization methods, such as Cross Validation, can be used to find the Kernel function that performs the best. \n",
    "However, since with Kernel trick there is no additional computation for separating data points in some high-dimension or infinite dimension, people just go with the infinite dimension by using the Gaussian (RBF) Kernel.\n",
    "##### RBF is the most commonly used Kernel. It has a tuning hyperparameter σ that is tuned to choose between a smooth or curvy boundary. In short, as a rule of thumb, once you realize linear boundary is not going to work try a non-linear boundary with an RBF Kernel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Kernel\n",
    "Linear kernel documentation – linear kernel source code – how to create SVMs in .NET with Accord.NET\n",
    "The Linear kernel is the simplest kernel function. It is given by the inner product <x,y> plus an optional constant c. Kernel algorithms using a linear kernel are often equivalent to their non-kernel counterparts, i.e. KPCA with linear kernel is the same as standard PCA.\n",
    "\n",
    "#### k(x, y) = x^T y + c\n",
    "\n",
    "### Polynomial Kernel\n",
    "The Polynomial kernel is a non-stationary kernel. Polynomial kernels are well suited for problems where all the training data is normalized.\n",
    "\n",
    "####  k(x, y) = (alpha x^T y + c)^d \n",
    "Adjustable parameters are the slope alpha, the constant term c and the polynomial degree d.\n",
    "\n",
    "### Gaussian Kernel\n",
    "The Gaussian kernel is an example of radial basis function kernel.\n",
    "\n",
    "####  k(x, y) = expleft(-frac{ lVert x-y rVert ^2}{2sigma^2}right) \n",
    "\n",
    "Alternatively, it could also be implemented using\n",
    "\n",
    "####  k(x, y) = expleft(- gamma lVert x-y rVert ^2 ) \n",
    "\n",
    "The adjustable parameter sigma plays a major role in the performance of the kernel, and should be carefully tuned to the problem at hand. If overestimated, the exponential will behave almost linearly and the higher-dimensional projection will start to lose its non-linear power. In the other hand, if underestimated, the function will lack regularization and the decision boundary will be highly sensitive to noise in training data.\n",
    "\n",
    "### Exponential Kernel\n",
    "The exponential kernel is closely related to the Gaussian kernel, with only the square of the norm left out. It is also a radial basis function kernel.\n",
    "\n",
    "####  k(x, y) = expleft(-frac{ lVert x-y rVert }{2sigma^2}right) \n",
    "\n",
    "### Laplacian Kernel\n",
    "The Laplace Kernel is completely equivalent to the exponential kernel, except for being less sensitive for changes in the sigma parameter. Being equivalent, it is also a radial basis function kernel.\n",
    "\n",
    "#### k(x, y) = expleft(- frac{lVert x-y rVert }{sigma}right)\n",
    "\n",
    "It is important to note that the observations made about the sigma parameter for the Gaussian kernel also apply to the Exponential and Laplacian kernels.\n",
    "\n",
    "### ANOVA Kernel\n",
    "The ANOVA kernel is also a radial basis function kernel, just as the Gaussian and Laplacian kernels. It is said to perform well in multidimensional regression problems (Hofmann, 2008).\n",
    "\n",
    "#### k(x, y) =  sum_{k=1}^n  exp (-sigma (x^k - y^k)^2)^d\n",
    "\n",
    "### Hyperbolic Tangent (Sigmoid) Kernel\n",
    "The Hyperbolic Tangent Kernel is also known as the Sigmoid Kernel and as the Multilayer Perceptron (MLP) kernel. The Sigmoid Kernel comes from the Neural Networks field, where the bipolar sigmoid function is often used as an activation function for artificial neurons.\n",
    "\n",
    "#### k(x, y) = tanh (alpha x^T y + c) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
